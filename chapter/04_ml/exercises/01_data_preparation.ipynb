{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eba2fee",
   "metadata": {},
   "source": [
    "Lecture: AI I - Basics \n",
    "\n",
    "Previous:\n",
    "[**Chapter 4.1: Data Preparation with scikit-learn**](../01_data_preparation.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c232848e",
   "metadata": {},
   "source": [
    "# Exercise 4.1: Data Preparation with scikit-learn\n",
    "\n",
    "- [Task 1: Missing Value Imputation](#task-1-missing-value-imputation)\n",
    "- [Task 2: Data Scaling](#task-2-data-scaling)\n",
    "- [Task 3: Building a Preprocessing Pipeline](#task-3-building-a-preprocessing-pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2680f46",
   "metadata": {},
   "source": [
    "> Hint: When doing the exercises put your solution in the designated \"Solution\" section:\n",
    "> ```python\n",
    "> # Solution (put your code here)\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdae188",
   "metadata": {},
   "source": [
    "## Task 1: Missing Value Imputation\n",
    "\n",
    "Missing data is common in real-world datasets and can break machine learning algorithms. You'll practice different imputation strategies to handle missing values.\n",
    "\n",
    "a) Use `SimpleImputer` to replace missing values with the mean and store it in `mean_imputed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca63429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites (don't edit this block)\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "np.random.seed(42)\n",
    "data_with_nan = np.array([\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [4.0, np.nan, 6.0],\n",
    "    [7.0, 8.0, np.nan],\n",
    "    [np.nan, 11.0, 12.0],\n",
    "    [13.0, 14.0, 15.0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef7f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (put your code here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9b50af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case (don't edit this block)\n",
    "assert ~np.isnan(mean_imputed).any()\n",
    "assert mean_imputed[1, 1] == 8.75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a3ec02",
   "metadata": {},
   "source": [
    "b) Use `SimpleImputer` to replace missing values with the median and store it in `median_imputed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6940268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (put your code here)\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "median_imputed = median_imputer.fit_transform(data_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adadaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case (don't edit this block)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9bbafd",
   "metadata": {},
   "source": [
    "c) Use `SimpleImputer` to replace missing values with a constant value (`0`) and store it in `constant_imputed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bc981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (put your code here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a46f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case (don't edit this block)\n",
    "assert ~np.isnan(mean_imputed).any()\n",
    "assert constant_imputed[1, 1] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ec1ace",
   "metadata": {},
   "source": [
    "## Task 2: Data Scaling\n",
    "\n",
    "Features with different scales (e.g., height in meters vs. age in years) can bias machine learning algorithms. Scaling ensures all features contribute equally to the learning process.\n",
    "\n",
    "a) Apply `StandardScaler` to normalize the data (mean=0, std=1) and store it in `standard_scaled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11fd2c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites (don't edit this block)\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "sample_data = np.array([\n",
    "    [1.75, 70.5, 25], \n",
    "    [1.80, 85.2, 30],\n",
    "    [1.65, 58.9, 22],\n",
    "    [1.90, 95.1, 35],\n",
    "    [1.70, 68.3, 28]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28374991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (put your code here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34e49c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case (don't edit this block)\n",
    "standard_scaled\n",
    "assert np.isclose(np.min(standard_scaled), -1.355261854357877)\n",
    "assert np.isclose(np.max(standard_scaled), 1.6274669424134713)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee940d7d",
   "metadata": {},
   "source": [
    "b) Apply `MinMaxScaler` to scale data to range [0, 1] and store it in `minmax_scaled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb6bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (put your code here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9451378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case (don't edit this block)\n",
    "assert np.isclose(np.min(minmax_scaled), 0)\n",
    "assert np.isclose(np.max(minmax_scaled), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d59a8c1",
   "metadata": {},
   "source": [
    "c) Apply `MinMaxScaler` to scale data to range [-1, 1] and store it in `minmax_neg_scaled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983cd6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (put your code here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c5ed1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case (don't edit this block)\n",
    "assert np.isclose(np.min(minmax_neg_scaled), -1)\n",
    "assert np.isclose(np.max(minmax_neg_scaled), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa691a8",
   "metadata": {},
   "source": [
    "## Task 3: Building a Preprocessing Pipeline\n",
    "\n",
    "Real-world data preprocessing involves multiple steps. Pipelines chain transformations together, ensuring consistent preprocessing and preventing data leakage between training and test sets. Also, real datasets often mix numerical and categorical features that need different preprocessing. ColumnTransformer lets you apply different transformations to different column types simultaneously.\n",
    "\n",
    "a) Use `Pipeline` from `sklearn.pipeline` with three named steps:\n",
    "   - `'imputer'`: `SimpleImputer(strategy='mean')`\n",
    "   - `'scaler'`: `StandardScaler()`\n",
    "Store in variable `preprocessing_pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d404670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites (don't edit this block)\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d64a418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (put your code here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5469a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case (don't edit this block)\n",
    "np.random.seed(42)\n",
    "raw_data = np.array([\n",
    "    [1.75, 70.5, np.nan],\n",
    "    [np.nan, 85.2, 30],\n",
    "    [1.65, np.nan, 22],\n",
    "    [1.90, 95.1, 35],\n",
    "    [1.70, 68.3, 28],\n",
    "    [1.82, 72.1, np.nan],\n",
    "    [1.68, 61.4, 26],\n",
    "    [np.nan, 88.9, 32]\n",
    "])\n",
    "\n",
    "transformed_data = preprocessing_pipeline.fit_transform(raw_data)\n",
    "assert ~np.isnan(transformed_data).any()\n",
    "assert np.isclose(np.min(transformed_data), -1.887677947616136)\n",
    "assert np.isclose(np.max(transformed_data), 2.0044593143431815)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3202992d",
   "metadata": {},
   "source": [
    "b) Define a ColumnTransformer with the following transformations:\n",
    "- for numerical features (`age`, `salary`, `experience`), apply the `preprocessing_pipeline` from task 3 and name it `'num'`.\n",
    "- for categorical features (`department`), apply `OneHotEncoder()` and name it `'cat'`.\n",
    "Store in variable `column_transformer`.\n",
    "\n",
    "Additionally, fit and transform the `data` using `column_transformer` and store the result in `transformed_mixed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29bc84c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites (don't edit this block)\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "mixed_data = pd.DataFrame({\n",
    "    'age': [25, 30, 35, 22, 28, 33],\n",
    "    'salary': [50000, 60000, 75000, 45000, 55000, 70000],\n",
    "    'department': ['IT', 'HR', 'IT', 'Finance', 'HR', 'IT'],\n",
    "    'experience': [2.5, 5.0, 8.5, 1.0, 3.5, 6.0]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e58519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (put your code here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b453cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case (don't edit this block)\n",
    "feature_names = column_transformer.get_feature_names_out()\n",
    "assert len(feature_names) == 6\n",
    "\n",
    "df = pd.DataFrame(transformed_mixed, columns=feature_names)\n",
    "assert df[\"cat__department_Finance\"].sum() == 1\n",
    "assert df[\"cat__department_HR\"].sum() == 2\n",
    "assert df[\"cat__department_IT\"].sum() == 3\n",
    "\n",
    "assert not df[\"num__age\"].isnull().any()\n",
    "assert not df[\"num__salary\"].isnull().any()\n",
    "assert not df[\"num__experience\"].isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c57ef66",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Lecture: AI I - Basics \n",
    "\n",
    "Next: [**Chapter 4.2: Machine Learning with scikit-learn**](../04_ml/02_machine_learning.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-lecture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
